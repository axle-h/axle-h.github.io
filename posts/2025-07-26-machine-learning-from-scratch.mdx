---
title: Machine Learning From Scratch
categories: [ai]
---

The recent AI boom started around 2017 with the paper ["Attention Is All You Need"](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need),
which introduced the [Transformer Architecture](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)),
making models like [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT) possible (the `T` in `GPT` stands for transformer).
This is groundbreaking stuff but is still based on the feedforward neural network, which was cutting edge in the 1970s and is literally just (lots of) tensor multiplication.

For modern (laptop scale) machine learning, it is usual to depend on a library such as [PyTorch](https://pytorch.org/).
These libraries have massively lowered the barrier to entry into machine learning by abstracting away the mathematics behind it, having you running big models on your GPU with little effort.
This is great as anyone with basic Python programming skills and the ability to follow a tutorial (or effectively prompt an LLM), can get started with machine learning.
I've had a play with PyTorch but left feeling I'd learned more of the abstraction than the underlying concepts.

As a learning exercise and of course *for fun*, I decided to go back to basics and implement my own machine learning from scratch.

## Target

First I needed a target, preferably a game, since... games are fun.
I've been making low level games in Rust: <PostLink name="rust-games" />.
They're actually perfect for this project as:

* There is no engine (like [Unity](https://unity.com/) or [Godot](https://godotengine.org/)) so I don't have to worry about bindings or non-machine learning relevant baggage getting in the way.
* I have direct access to the game loop so integrating an agent into the game is dead easy:
  ```rust
  fn game_loop() {
      let mut game = Game {};
      let mut agent = AiAgent {};
      loop {
          // let input = read_human_controls(); ‚Üê humans need not apply
          let input = agent.act(&game); // ‚Üê the agent is playing instead
          update_game_state(&mut game, input);
          draw_game(&game);
      }
  }
  ```
* Rust is so fast that I don't need perfectly optimised code for it to be performant.

I chose a Tetris clone [rustris](https://github.com/axle-h/rustris) for this project.

## Tetris

The game of Tetris is played with a *tetromino* that falls from the top of a *board*.
When the *tetromino* touches the bottom of the board, or the *stack*, it will enter a lock state,
where after some timeout and without action, it will lock into place and become part of the stack.
The stack is cleared by arranging lines.
The maximum number of lines that can be cleared in a single move is 4, which is called a *Tetris*.
Points are added for various actions and outcomes in the game.
The game is lost when the stack reaches the top of the board.
It is expected that every game will eventually be lost, games are classified by the number of lines cleared and the total points scored.

Every tetromino has a name:

<Box maxW={100} flex={1} alignItems="center" justifyContent="middle">
| Name | Tetromino   |
|------|-------------|
| `O`  | ![O](O.png) |
| `Z`  | ![Z](Z.png) |
| `S`  | ![S](S.png) |
| `T`  | ![T](T.png) |
| `L`  | ![L](L.png) |
| `J`  | ![J](J.png) |
| `I`  | ![I](I.png) |
</Box>

## API

First I need to design the API of the agent. A Tetris playing agent will consume game state to produce actions:

<PintoraDiagram src={`
componentDiagram
() "Game State" as state
[Agent] as agent
() "Actions" as actions
state --> agent
agent --> actions
`} />

### State

Naively, and with a cluster of GPUs, we would provide the model with raw board state.
The Tetris board is 10x20 which is 2<sup>200</sup> = 1.6 * 10<sup>60</sup> distinct states.
Whilst the model having access to this massive amount of raw state would provide the best chance of complex strategies emerging, this scale is unfortunately out of scope.
I am looking for something more laptop scale than RTX 5090.
I am also **much** more interested in developing the model from scratch than having something cutting edge.

So I decided to pre-optimise the game state into "features".

## Features

| Feature                           | Description                                                                                                                                                                                                                                   |
|-----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![Stack Height](stack-height.png) | **Maximum Stack Height** <br /> e.g. 3                                                                                                                                                                                                        |
| ![Roughness](roughness.png)       | **Roughness** <br /> e.g. 2+1+2+1+1+1+2 = 10 <br /> To express the "flatness" of the stack. I played a **LOT** of Tetris for this project and realised that 5/7 Tetromino have a flat side so the optimal stack shape is as flat as possible. |
| ![Open Hole](open-hole.png)       | **Open Hole** <br /> One or more empty blocks with a stack block above but not on either side. <br /> Can be filled by sliding a Tetromino.                                                                                                   |
| ![Closed Hole](closed-hole.png)   | **Closed Hole** <br /> One or more empty blocks completely surrounded by stack or walls. <br /> Can only be filled by removing stack blocks above.                                                                                            |
| ![Pillar](pillar.png)             | **Pillar** <br /> A gap 3 or more blocks tall that can only be filled with an I Tetromino.                                                                                                                                                    |

I take the delta of each feature for each available tetromino placement.
I think this is important as the model will learn much better when for every action it takes, no matter the existing state of the board, the input parameters will change at the same scale.
E.g. without taking the delta, if there are 15 holes already and placing a tetromino creates another hole, then the hole feature will change by an amount that is 15x less than what would have been observed on an empty board.

I also tell the model when:
* It has covered a hole.
* It has cleared a line.
* It has cleared a Tetris (4 lines).
* Tetromino altitude - the max y-coordinate of the final position of the tetromino.

### Action

Another scaling issue applies to how actions are interpreted from the model output.
Tetris uses a spatial control scheme Left, Right, Rotate.
Humans can naturally interpret a desired placement, for example slotting an `I` tetromino into a gap in the stack, as a series of Left, Right, Rotate commands.

If we expect a model to generate output like this, it must be expected that it has to dedicate significant resources to learning the spatial nature of the game.
I am sure that I have a low chance of pulling off something sophisticated enough to handle spatial reasoning.
Instead, I will use an algorithm that, given the current state of the game, generates all possible placements available and provides them to the model summarised by their features.

## Move Search

The move search algorithm is a breadth-first search over all possible tetromino placements.
It explores all possible placements of the current tetromino by taking every valid move that results in a new final (locked) position.
This borders on brute-force but is feasible as the number of placements is limited by the board size.
There are only 17 possible placements of the `I` tetromino on an empty board for example.

<PintoraDiagram src={`
activityDiagram
start
:start with the Tetromino starting position;
while (any starting positions left ?) is (yes)
  :take next move;
  :drop to floor or stack;
  if (is new end position ?) then
    :add move to results;
    :from initial position\nmove left, right & rotate;
    :add new starting positions;
  else (no)
    :discard move;
  endif
endwhile (no)
end
`} />


The move search algorithm is then run again with the final positions of the first run as the new starting positions.
This is to explore any positions that can only be found by moving the tetromino during the lock phase,
for example sliding a tetromino left or right into an open hole.

This entire process is run once for the current tetromino and then again for the 'held' tetromino.

## Linear Model

At this point, I was ready to implement a model.
I had a load of code that was unit tested (of course) but not integrated, so I chose to start with the simplest possible model, a linear function over all the features.

<Latex src={`
  Q = P_1\\delta StackHeight + P_2\\delta Roughness + P_3\\delta OpenHole + P_4\\delta ClosedHole + P_5\\delta Pillar
`} />

> Where `Q` is the quality of the move and `P` are the parameters to be learned.

<CheckList>
  <CheckListItem>It works surprisingly well!</CheckListItem>
  <CheckListItem>The model parameters are transparent, they can easily be sense checked e.g. I expect holes to be a good indicator of bad play, so I therefore expect the hole coefficient to be highly negative.</CheckListItem>
  <XListItem>The model is incapable of learning complicated, non-linear strategies e.g. play safe when the stack is high.</XListItem>
</CheckList>

## Agent

With a simple model in place, I was ready to implement the agent.
The agent, running directly in the game loop, will:

1. With the game state, run the move search algorithm to get all possible placements
2. Use the model to qualify each placement
3. Take game actions to execute the best placement (e.g. left, rotate, drop)

## Linear Model Training

The linear model only has 10 parameters to learn, so I could actually hand-pick them and get a fairly decent result.
The model will get more complex though so let's do some machine learning anyway.

### Supervised learning

The natural choice for a linear model would be linear regression.
Linear regression is a supervised machine learning technique that relies on labelled data to (basically) plot a line of best fit for each statistically significant component.
Think `y = mx + c` from GCSE.

I used to work in credit reference so am pretty familiar with the technique (your credit score is literally just a linear regression over your credit reference data).
For supervised learning, I needed to label the data. My options are:

* **Record my gameplay**, labelling my actions as `good` moves.
  This, whilst maybe fun to achieve, would mean that the model would be biased towards my decisions rather than the actual 'best' decisions.
  I have played a **LOT** of Tetris, but I am not a Tetris master, I probably make hundreds of mistakes in each game and this bias would be learnt by the model.
* **Collect professional Tetris gameplay**, labelling professional player actions as `good` moves.
  There are high quality recordings of professional Tetris gameplay available online.
  I would have to write a tool to extract the game state and actions from the video.
  Whilst I think this could work well, it would be a lot of work, so I have put it aside for now.

> Also: even professionals make mistakes.

### Reinforcement learning

Reinforcement learning is a machine learning technique where an agent is free to act in the environment and is given a reward, positive or negative, for each action it takes.
The model learns to choose the actions that maximise cumulative reward.

I was not overly enthusiastic about reinforcement learning for this project:

I did not want to define a detailed reward function as this risks influencing behaviours based on preconceptions.
E.g. if I defined a reward for clearing lines that was maybe slightly too high, then the model would learn to clear lines at the expense of all other strategies.

I briefly considered a broader reward function that worked retrospectively.
E.g. when a game is lost, all placements leading up to the final placement before loss are penalised, whereas placements leading up to a Tetris clear are rewarded.
I decided to skip this for now though as it seemed like it would be a mess of hyperparameters and the algorithms weren't massively clear to implement from scratch.
I will definitely come back to this later though, maybe as an optimisation step for a model pre-trained with unsupervised learning.

### Unsupervised learning

Unsupervised learning is a machine learning technique used to find patterns or structure in unlabeled data.
It effectively searches for a point in the multidimensional space over the model parameters that maximises some reward function.

I decided to go with this approach to start because:
* Tetris has a native reward function that is a good fit: the game score.
* This technique requires a lot of data but my Rust implemented game can simulate multiple hours of gameplay per second.
  My laptop won't be happy with it, but it shouldn't be a problem playing many thousands of games over a typical training run.
* The unsupervised algorithms are fun to implement e.g. a genetic algorithm is based on evolution.

### Genetic algorithm

A genetic algorithm is an unsupervised learning technique, mimicking the process of evolution.
It acts like a search heuristic for maximising some reward function over the model parameters.

* **Survival of the fittest:** only the best (fittest) solutions will survive to have a chance of selection.
* **Selection:** the fittest solutions are weighted by their fitness and merged into new solutions.
* **Mutation:** some solutions are randomly changed to create new solutions.

Like real evolution, it has no concept of gradient decent, it is literally picking random solutions over multiple generations and mutating them in a process that over time (hopefully) causes a "population" to converge on an optimal solution.

<PintoraDiagram src={`
activityDiagram
start
:start with a population of random model parameters;
while (population not converged ?) is (yes)
  :evaluate the fitness of each individual;
  :select the fittest individuals;
  :crossover the fittest individuals to create new individuals;
  :mutate some individuals;
endwhile (no)
end
`} />

### Training stability

In Tetris, the next tetromino is randomly chosen from a "bag" containing all 7 shapes.
Once the bag is empty, the tetromino shapes are placed back in the bag and the bag is shuffled.
This makes a Tetris playing model unstable, which is especially an issue for unsupervised learning algorithms, which rely on (mostly) random chance to find an optimal model.
If the models are unstable, then training is less likely to converge on something optimal.
**TLDR;** the randomness of the game means that the quality of the optimal model can deteriorate over time. Whereas we want the opposite!

To solve this, I first seed the random number generator with a random (time based) value, then I rotate the seed every 100 generations.
This gives the training algorithm chance to converge on a model for a single deterministic game.
My hope is that over time, the most generally optimised models will be selected and the training algorithm will converge on a model that is stable across many games.

## Linear Results

I trained the linear model with a genetic algorithm for 700 generations of 1000 models each.

### Linear Coefficients

Here are the coefficients of the linear model after training:

<BarChart
  data={[
      { feature: 'Hole Cover', coefficient: -0.977903 },
      { feature: 'Closed Holes', coefficient: -0.937491 },
      { feature: 'Open Holes', coefficient: -0.842758 },
      { feature: 'Pillars', coefficient: -0.459028 },
      { feature: 'Roughness', coefficient: -0.086589 },
      { feature: 'Line Clear', coefficient: -0.050854 },
      { feature: 'Altitude', coefficient: -0.039745 },
      { feature: 'Stack Height', coefficient: 0.031847 },
      { feature: 'Tetris Clear', coefficient: 0.773208 },
  ]}
  series="coefficient"
/>

As expected, the model has learned that:
* Covering a hole is the worst thing you can do as it makes clearing the hole more difficult.
* Closed holes are bad.
* Open holes are bad but not as bad as closed holes, since you can sometimes fill them with a sliding tetromino.
* Pillars are bad as they can only be filled with an `I` tetromino.
* Tetris clears are good.
* All the other coefficients are in a fine balance as they all interact with each other.
  E.g. you would imagine that clearing a line would be a good action to take, but the model actually applies a small penalty for a line clear, which seems weird in isolation.
  This looks to be offset by a small preference for a taller stack.
  Maybe this combination of coefficient values work better for some stack arrangements than others.

### Linear Performance

Here's the plot of the best score over 700 generations:

<LineChart
  data={[
    { "Generation": 1, "Score": 420, "Trend": 0 },
    { "Generation": 10, "Score": 9912, "Trend": 0 },
    { "Generation": 20, "Score": 26576, "Trend": 653953.585737325 },
    { "Generation": 30, "Score": 43678, "Trend": 2319516.17168045 },
    { "Generation": 40, "Score": 91926, "Trend": 3486457.79247964 },
    { "Generation": 50, "Score": 169886, "Trend": 4385530.53995922 },
    { "Generation": 60, "Score": 511057, "Trend": 5117039.39219749 },
    { "Generation": 70, "Score": 3272749, "Trend": 5733738.80027938 },
    { "Generation": 80, "Score": 7679609, "Trend": 6266825.78918789 },
    { "Generation": 90, "Score": 8974123, "Trend": 6736289.47328297 },
    { "Generation": 100, "Score": 12652447, "Trend": 7155710.13688435 },
    { "Generation": 110, "Score": 7362719, "Trend": 7534736.30061993 },
    { "Generation": 120, "Score": 9065946, "Trend": 7880469.78740273 },
    { "Generation": 130, "Score": 9065946, "Trend": 8198289.94667908 },
    { "Generation": 140, "Score": 9065946, "Trend": 8492369.25197079 },
    { "Generation": 150, "Score": 9065946, "Trend": 8766009.33771748 },
    { "Generation": 160, "Score": 9065946, "Trend": 9021867.65356839 },
    { "Generation": 170, "Score": 9065946, "Trend": 9262114.86278039 },
    { "Generation": 180, "Score": 9065946, "Trend": 9488546.92275889 },
    { "Generation": 190, "Score": 9065946, "Trend": 9702666.65624794 },
    { "Generation": 200, "Score": 9245238, "Trend": 9905744.26661282 },
    { "Generation": 210, "Score": 7579849, "Trend": 10098863.0004428 },
    { "Generation": 220, "Score": 13575286, "Trend": 10282954.1280198 },
    { "Generation": 230, "Score": 13575286, "Trend": 10458824.1068211 },
    { "Generation": 240, "Score": 13575286, "Trend": 10627175.9348792 },
    { "Generation": 250, "Score": 13575286, "Trend": 10788626.1243332 },
    { "Generation": 260, "Score": 13575286, "Trend": 10943718.3308407 },
    { "Generation": 270, "Score": 13575286, "Trend": 11092934.3996051 },
    { "Generation": 280, "Score": 13575286, "Trend": 11236703.3942207 },
    { "Generation": 290, "Score": 13575286, "Trend": 11375409.0348462 },
    { "Generation": 300, "Score": 13575286, "Trend": 11509395.8705801 },
    { "Generation": 310, "Score": 10863808, "Trend": 11638974.4360416 },
    { "Generation": 320, "Score": 10863808, "Trend": 11764425.5863859 },
    { "Generation": 330, "Score": 10863808, "Trend": 11886004.1629897 },
    { "Generation": 340, "Score": 10863808, "Trend": 12003942.1101137 },
    { "Generation": 350, "Score": 10863808, "Trend": 12118451.1383559 },
    { "Generation": 360, "Score": 10863808, "Trend": 12229725.011748 },
    { "Generation": 370, "Score": 10863808, "Trend": 12337941.5205635 },
    { "Generation": 380, "Score": 10863808, "Trend": 12443264.1902792 },
    { "Generation": 390, "Score": 10863808, "Trend": 12545843.7679354 },
    { "Generation": 400, "Score": 10863808, "Trend": 12645819.5198064 },
    { "Generation": 410, "Score": 6398523, "Trend": 12743320.368416 },
    { "Generation": 420, "Score": 10918397, "Trend": 12838465.892189 },
    { "Generation": 430, "Score": 10918397, "Trend": 12931367.2071851 },
    { "Generation": 440, "Score": 10918397, "Trend": 13022127.747221 },
    { "Generation": 450, "Score": 10918397, "Trend": 13110843.9561182 },
    { "Generation": 460, "Score": 12253918, "Trend": 13197605.9036914 },
    { "Generation": 470, "Score": 12288621, "Trend": 13282497.835344 },
    { "Generation": 480, "Score": 13593990, "Trend": 13365598.6636739 },
    { "Generation": 490, "Score": 13593990, "Trend": 13446982.4092838 },
    { "Generation": 500, "Score": 13593990, "Trend": 13526718.5969642 },
    { "Generation": 510, "Score": 9448686, "Trend": 13604872.6125654 },
    { "Generation": 520, "Score": 15850174, "Trend": 13681506.0251466 },
    { "Generation": 530, "Score": 15850222, "Trend": 13756676.8783799 },
    { "Generation": 540, "Score": 15850222, "Trend": 13830439.9546647 },
    { "Generation": 550, "Score": 15850222, "Trend": 13902847.0149621 },
    { "Generation": 560, "Score": 15850222, "Trend": 13973947.0169798 },
    { "Generation": 570, "Score": 15850222, "Trend": 14043786.314012 },
    { "Generation": 580, "Score": 15850222, "Trend": 14112408.8364547 },
    { "Generation": 590, "Score": 15850222, "Trend": 14179856.2577791 },
    { "Generation": 600, "Score": 15850222, "Trend": 14246168.1465317 },
    { "Generation": 610, "Score": 15019605, "Trend": 14311382.1057495 },
    { "Generation": 620, "Score": 15897722, "Trend": 14375533.9010209 },
    { "Generation": 630, "Score": 15897722, "Trend": 14438657.5782842 },
    { "Generation": 640, "Score": 15897722, "Trend": 14500785.5723339 },
    { "Generation": 650, "Score": 15897722, "Trend": 14561948.806903 },
    { "Generation": 660, "Score": 15897722, "Trend": 14622176.7870909 },
    { "Generation": 670, "Score": 15897722, "Trend": 14681497.6848325 },
    { "Generation": 680, "Score": 15897722, "Trend": 14739938.4180248 },
    { "Generation": 690, "Score": 15897722, "Trend": 14797524.7238691 },
    { "Generation": 700, "Score": 15897722, "Trend": 14854281.2269291 }
  ]}
  x="Generation"
  y="Score"
  trend="Trend"
/>

Observations:

* You can see the dips in quality at every 100th generation, this is when the random seed is changed.
* The model seems to stagnate before each dip.
* After the dip, the model performance often recovers to a greater level than the previously stagnated performance.
  This must mean that the model is learning a more generalised strategy that is not biased towards a particular seeded game.
* The model converges on a score of around 16 million points, which is pretty close to the
  [current classic NES world record of 16.7 million points](https://www.ndtv.com/feature/15-year-old-gamer-breaks-6-tetris-world-records-with-the-highest-ever-recorded-score-5566253).
* The trend looks logarithmic i.e. longer training runs probably won't improve the performance of the model much more.

## Neural Network Model

With the linear model working, it's parameters sense checked, the bugs fixed in the genetic algorithm and the agent proven, I was ready to try a more complex model.
I needed something non-linear that could potentially learn complex strategies, so I went with a neural network.
To keep the existing, proven agent structure, I needed the network to be a direct replacement for the linear model, with the exact same inputs and outputs.
The input layer must have one neuron per game feature and the output layer must have a single neuron; the quality of the move.

![neural network](neural-network.svg)

The whole point of this is to provide a model architecture allowing for non-linear behaviour to emerge, so:
* I give the model the delta of each feature **AND** the absolute values, so it has the context of the current action, plus the global state of the game available.
* I use multiple hidden layers to allow the model to learn complex relationships between the features.
* I use a non-linear activation function, the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).

The neural network topology I settled on is:

* Input layer with 20 neurons
* 2 fully connected hidden layers with 20 neurons each
* Fully connected output layer with 1 neuron

This totals to 1,281 parameters. For reference [GPT-4 has 1.8 trillion parameters](https://www.kdnuggets.com/2023/07/gpt4-details-leaked.html), so this is a very small model.

### Hold on, what is a neural network exactly?

> Of course, I could have used a library here like [burn](https://github.com/tracel-ai/burn) (remember I'm using Rust so PyTorch is not a practical option)
  but I wanted to implement a neural network from scratch, as a learning experience.

In my model, a neuron is a mathematical function that:

1. Applies a weighted sum over input values, mostly the results from other neurons.
2. Adds a constant value to the result, so that an activation bias is possible.
3. Passes the result through an activation function, which introduces non-linearity into the neuron.

Therefore, each neuron has two sets of parameters:

* A weight for each input value.
* A single bias value.

Neurons are arranged in layers:
1. The input layer, which takes the model input parameters.
2. One or more hidden layers, which allow the model to build increasingly complex abstractions over the input parameters.
3. The output layer, which produces the model output.

The networks I have implemented are fully connected feed-forward networks.
That means that every neuron in one layer is connected to every neuron in the next layer.
There are decent strategies for learning a more optimal network topology, such as [NEAT (NeuroEvolution of Augmenting Topologies)](https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies), but I decided to let the training process decide which connections are not important.
In training, the weight of useless connections should tend towards 0, which effectively removes them from the model.

This is actually a simplification as it means that I can implement the network with A-level grade matrix multiplication.
For illustration, given a simple network with two layers:

![neural network](neural-network-2.svg)

Then the output of the network can be expressed as a matrix multiplication of the weights and inputs, plus the biases, all passed through the activation function:

<Latex src={`
\\begin{bmatrix}
  x \\\\
  y
\\end{bmatrix}
= \\sigma \\left(

\\begin{bmatrix}
a_x & b_x & c_x \\\\
a_y & b_y & c_y
\\end{bmatrix}

\\otimes

\\begin{bmatrix}
a \\\\
b \\\\
c
\\end{bmatrix}
+
\\begin{bmatrix}
x_{\\text{bias}} \\\\
y_{\\text{bias}}
\\end{bmatrix}

\\right)
`} />

Doing some A-level homework, this can be multiplied out into components:

<Latex src={`
x = \\sigma(a_x a + b_x b + c_x c + x_{\\text{bias}}) \\\\
y = \\sigma(a_y a + b_y b + c_y c + y_{\\text{bias}})
`} />

Ignoring transformers and the attention mechanism, modern LLMs are literally this but turned up to 11.
They're using trillions of parameters over hundreds of layers but at their lowest level, they are simply tensor (n-dimensional) multiplication.
It is amazing that reasoning seems to emerge from something so mathematically simple.

## Neural Network Training

It is usual to train a neural network with a supervised learning technique such as backpropagation.
I did implement backpropagation to test my neural network implementation, but the technique is not applicable, as I lack good quality labeled data.

Since the neural network model is a direct replacement for the linear model, I can use the exact same unsupervised genetic algorithm to train it.
I appreciate that with this many parameters, unsupervised learning is not the most efficient machine learning technique, as a lot of compute is required to traverse such a large search space, but I thought it was worth a shot anyway.
Maybe I will get lucky...

> At this point, I made a tweak to the genetic algorithm to stop rotating the seed every 100 generations and instead rotate it every 10 stale generations, where the score did not improve.
  The idea behind this was to prevent the model from stagnating for too long.

## Neural Results

I can't really show how the neural network has learned like I did with the linear model, as it is effectively a black box.
There are techniques I can use to visualise activation patterns in the network to maybe derive an idea of what's going on, but I haven't done that yet.

I trained this model for literally an entire weekend and boy am I glad I left it running that long!
I set a limit per game of 10,000 lines and at generation 7,750, after millions of games played, billions of in-game hours, the model **FINALLY** hit this limit with a score of 647,931,149 points!

<LineChart
  data={[
    { "Generation": 1, "Score": 1563 },
    { "Generation": 50, "Score": 198462 },
    { "Generation": 100, "Score": 1029210 },
    { "Generation": 150, "Score": 2472636 },
    { "Generation": 200, "Score": 6049373 },
    { "Generation": 250, "Score": 1947177 },
    { "Generation": 300, "Score": 1490057 },
    { "Generation": 350, "Score": 895691 },
    { "Generation": 400, "Score": 1418241 },
    { "Generation": 450, "Score": 705834 },
    { "Generation": 500, "Score": 1508576 },
    { "Generation": 550, "Score": 1436626 },
    { "Generation": 600, "Score": 986015 },
    { "Generation": 650, "Score": 1347594 },
    { "Generation": 700, "Score": 2581067 },
    { "Generation": 750, "Score": 9335011 },
    { "Generation": 800, "Score": 3511095 },
    { "Generation": 850, "Score": 2475585 },
    { "Generation": 900, "Score": 3036621 },
    { "Generation": 950, "Score": 5368769 },
    { "Generation": 1000, "Score": 7552850 },
    { "Generation": 1050, "Score": 549592 },
    { "Generation": 1100, "Score": 6794371 },
    { "Generation": 1150, "Score": 4725814 },
    { "Generation": 1200, "Score": 3267718 },
    { "Generation": 1250, "Score": 3313925 },
    { "Generation": 1300, "Score": 6295872 },
    { "Generation": 1350, "Score": 2163975 },
    { "Generation": 1400, "Score": 10529995 },
    { "Generation": 1450, "Score": 4113292 },
    { "Generation": 1500, "Score": 7663436 },
    { "Generation": 1550, "Score": 2604502 },
    { "Generation": 1600, "Score": 3467638 },
    { "Generation": 1650, "Score": 1540164 },
    { "Generation": 1700, "Score": 3202302 },
    { "Generation": 1750, "Score": 1810409 },
    { "Generation": 1800, "Score": 5013269 },
    { "Generation": 1850, "Score": 6113731 },
    { "Generation": 1900, "Score": 6198942 },
    { "Generation": 1950, "Score": 5573719 },
    { "Generation": 2000, "Score": 2329012 },
    { "Generation": 2050, "Score": 2407383 },
    { "Generation": 2100, "Score": 5109661 },
    { "Generation": 2150, "Score": 1990602 },
    { "Generation": 2200, "Score": 2744490 },
    { "Generation": 2250, "Score": 14070727 },
    { "Generation": 2300, "Score": 2959180 },
    { "Generation": 2350, "Score": 11982169 },
    { "Generation": 2400, "Score": 8367161 },
    { "Generation": 2450, "Score": 6789957 },
    { "Generation": 2500, "Score": 3949844 },
    { "Generation": 2550, "Score": 10878051 },
    { "Generation": 2600, "Score": 4552998 },
    { "Generation": 2650, "Score": 2289486 },
    { "Generation": 2700, "Score": 2165050 },
    { "Generation": 2750, "Score": 6796945 },
    { "Generation": 2800, "Score": 3588585 },
    { "Generation": 2850, "Score": 4250512 },
    { "Generation": 2900, "Score": 6446556 },
    { "Generation": 2950, "Score": 5279696 },
    { "Generation": 3000, "Score": 6007547 },
    { "Generation": 3050, "Score": 3166626 },
    { "Generation": 3100, "Score": 4655111 },
    { "Generation": 3150, "Score": 10819594 },
    { "Generation": 3200, "Score": 6756600 },
    { "Generation": 3250, "Score": 2570083 },
    { "Generation": 3300, "Score": 7279365 },
    { "Generation": 3350, "Score": 10164928 },
    { "Generation": 3400, "Score": 6710126 },
    { "Generation": 3450, "Score": 6692447 },
    { "Generation": 3500, "Score": 3570785 },
    { "Generation": 3550, "Score": 3034798 },
    { "Generation": 3600, "Score": 3794383 },
    { "Generation": 3650, "Score": 3742456 },
    { "Generation": 3700, "Score": 3396388 },
    { "Generation": 3750, "Score": 8582194 },
    { "Generation": 3800, "Score": 9814025 },
    { "Generation": 3850, "Score": 2148926 },
    { "Generation": 3900, "Score": 5810884 },
    { "Generation": 3950, "Score": 1940527 },
    { "Generation": 4000, "Score": 1804544 },
    { "Generation": 4050, "Score": 3269937 },
    { "Generation": 4100, "Score": 6289540 },
    { "Generation": 4150, "Score": 7340521 },
    { "Generation": 4200, "Score": 5884774 },
    { "Generation": 4250, "Score": 3126303 },
    { "Generation": 4300, "Score": 2849886 },
    { "Generation": 4350, "Score": 4238725 },
    { "Generation": 4400, "Score": 1030084 },
    { "Generation": 4450, "Score": 1324473 },
    { "Generation": 4500, "Score": 1960228 },
    { "Generation": 4550, "Score": 6651497 },
    { "Generation": 4600, "Score": 9815333 },
    { "Generation": 4650, "Score": 10547320 },
    { "Generation": 4700, "Score": 7533692 },
    { "Generation": 4750, "Score": 8212291 },
    { "Generation": 4800, "Score": 4329025 },
    { "Generation": 4850, "Score": 3257280 },
    { "Generation": 4900, "Score": 3085796 },
    { "Generation": 4950, "Score": 2001286 },
    { "Generation": 5000, "Score": 6903991 },
    { "Generation": 5050, "Score": 7673814 },
    { "Generation": 5100, "Score": 15012472 },
    { "Generation": 5150, "Score": 2643426 },
    { "Generation": 5200, "Score": 3297152 },
    { "Generation": 5250, "Score": 2505654 },
    { "Generation": 5300, "Score": 3604482 },
    { "Generation": 5350, "Score": 1648476 },
    { "Generation": 5400, "Score": 3659845 },
    { "Generation": 5450, "Score": 1597138 },
    { "Generation": 5500, "Score": 8843754 },
    { "Generation": 5550, "Score": 2854083 },
    { "Generation": 5600, "Score": 3315276 },
    { "Generation": 5650, "Score": 3392593 },
    { "Generation": 5700, "Score": 8406901 },
    { "Generation": 5750, "Score": 2791784 },
    { "Generation": 5800, "Score": 3389008 },
    { "Generation": 5850, "Score": 8392212 },
    { "Generation": 5900, "Score": 6729655 },
    { "Generation": 5950, "Score": 14868625 },
    { "Generation": 6000, "Score": 34536750 },
    { "Generation": 6050, "Score": 140493930 },
    { "Generation": 6100, "Score": 10368608 },
    { "Generation": 6150, "Score": 48119777 },
    { "Generation": 6200, "Score": 70698106 },
    { "Generation": 6250, "Score": 16176094 },
    { "Generation": 6300, "Score": 52881372 },
    { "Generation": 6350, "Score": 42406964 },
    { "Generation": 6400, "Score": 23537060 },
    { "Generation": 6450, "Score": 84767421 },
    { "Generation": 6500, "Score": 33068565 },
    { "Generation": 6550, "Score": 346157670 },
    { "Generation": 6600, "Score": 41211374 },
    { "Generation": 6650, "Score": 23624279 },
    { "Generation": 6700, "Score": 63362543 },
    { "Generation": 6750, "Score": 48452039 },
    { "Generation": 6800, "Score": 11474511 },
    { "Generation": 6850, "Score": 114359366 },
    { "Generation": 6900, "Score": 102050180 },
    { "Generation": 6950, "Score": 36784803 },
    { "Generation": 7000, "Score": 63180457 },
    { "Generation": 7050, "Score": 34660151 },
    { "Generation": 7100, "Score": 8706550 },
    { "Generation": 7150, "Score": 12580431 },
    { "Generation": 7200, "Score": 87977330 },
    { "Generation": 7250, "Score": 158558877 },
    { "Generation": 7300, "Score": 190751811 },
    { "Generation": 7350, "Score": 64514220 },
    { "Generation": 7400, "Score": 70400698 },
    { "Generation": 7450, "Score": 57926547 },
    { "Generation": 7500, "Score": 32599793 },
    { "Generation": 7550, "Score": 20928915 },
    { "Generation": 7600, "Score": 87132923 },
    { "Generation": 7650, "Score": 169557757 },
    { "Generation": 7700, "Score": 206158913 },
    { "Generation": 7750, "Score": 647931149 }
  ]}
  x="Generation"
  y="Score"
/>

There was definitely a breakthrough at around generation 6,000 that allowed the model to start scoring in the hundreds of millions of points.
This is amazing, but the massive lag before the breakthrough demonstrates the drawbacks of using unsupervised learning for a problem as complex as Tetris.
I'm not sure of the timing, but I think it took over 30 hours to get to generation 6,000.
I could have easily turned it off before then and missed out on this breakthrough.

## Demo

Using the same seed that produced the 10,000 line run, I let the model play for as long as possible.
It managed to play for 3 hours, clearing 10,840 lines and scoring 760,264,498 points.
I recorded the playthrough and uploaded it to YouTube for your viewing pleasure üòÄ:

<iframe className="youtube" src="https://www.youtube.com/embed/KRohCEPzE4Y?si=r1fviG2kq8r3bOe_" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerPolicy="strict-origin-when-cross-origin" allowFullScreen></iframe>

## Next Steps

I've managed to, somewhat luckily, grind superhuman performance out of a model-free unsupervised learning technique.
Where do I go from here?

### A better agent

You may have noticed that I have not documented any attempt to lookahead, classifying actions by their future consequences or opportunities.
That doesn't mean that I did not attempt to, just that my attempts were unsuccessful.

The issue I could not solve acceptably was the sheer amount of compute required.
For example, the `I` tetromino shape can be placed in 17 different positions on an empty board.
If the next tetromino is the `L` shape, then there are 35 further placements to be considered... for every one of the initial 17 placements.
Also, at each placement, you have to consider the held tetromino, further branching the tree.
This results in an exponentially increasing number of placements to consider for each level of lookahead.

The increased compute meant that training was significantly slower.
Coupled with inefficient unsupervised learning, this resulted in an unacceptable rate of convergence.

I tried to solve this by:

* **Pruning the tree of placements that were obviously bad.**
  I could not remove enough of them to make a difference and the heuristics I used were not free to compute, so I think in some cases I actually made it worse.
* **Training the model without lookahead and then "enlightening" it with the feature once deployed.**
  This seemed to cause the models to perform even worse than in training without lookahead enabled.
  My theory is that the models learned something about, or became reliant on the scales of the feature values.
  For example, in training the model was typically dealing with, "if I make this placement then I will create one hole", but then in deployment it might have had to deal with, "if I make this placement then I will create 10 holes".

I am not sure that lookahead is even required for a perfect Tetris strategy (one that can play forever) so it might not be worth the effort.
Maybe I might still revisit this later but with some serious pruning and a way to isolate it from the current model parameters.
One approach might be to provide the model with another, much more limited set of features for lookahead placements.

### Tuning model parameters

I can tune the model parameters to improve the performance of the model or redirect its behaviour towards something more desirable.

If you watched any of the demo video then you may have noticed some weird behaviour where `I` tetromino shapes are stacked up on the RHS column.
This was actually an experiment gone wrong where I added the height of the RHS column as a game feature, hoping that the model would learn to slam big Tetris clears by keeping it free üòé.
However, this didn't work out and the model actually learned to do the total opposite, resulting in the stacking of `I` tetromino on the RHS column instead ‚òπÔ∏è.
I am not sure of the evolutionary advantage that this technique provided.
Maybe it had no positive or negative effects so it was never actively selected against, and it was random chance that it remained in the final model?

I could either:

* Double down and give it the heights of every column, maybe then the model will derive its own features and abstractions over the hidden layers?
  This will probably require massive training runs with the genetic algorithm though as there will be a lot of noise to sift through.
* Derive a new feature that somehow describes the behaviour I want e.g. leaving a single column open for big Tetris clears.

### More intelligent unsupervised learning

The genetic algorithm I am using is basically brute force on steroids.

There is no intelligence in how mutation changes the weights so no guarantee that any particular mutation will result in a better model.
Maybe, via some gradient descent technique, I can bias mutation to move weights in the right direction more often?
I don't know, I haven't seen anything on the internet that discusses this in depth.

There are other, much better documented, techniques available to level up the genetic algorithm.
For example, I could introduce speciation to allow it to explore multiple strategies at once.

### Longer training runs

Every single game of Tetris ever played by every model I've ever trained has ended in a game over.

That must mean that the model still hasn't gotten good enough to play a perfect game of Tetris.
I've not got any proof that a perfect game is possible, but I suspect that it is possible to play for much longer than 10,000 lines.

The unsupervised learning process had a massive breakthrough after many hours of training, so maybe it might make another one after a longer training run?

### Optimizing for Score

The models still haven't learnt the canonical strategies of master Tetris players, who tend to optimize for Tetris clears.
I think the reason for this is that for any non-Tetris optimized strategy, a longer game will usually result in a higher score.
So a survival orientated strategy has received a higher selection pressure over more risky techniques that may have resulted in higher scores if they were explored further.

I think I can address this by taking a survival optimized model and then either:

* Training it with a supervised learning technique to optimize for Tetris clears or to mimic professional play styles.
* Further training it with unsupervised learning but limit game time to force more efficient scoring strategies.

## The Code

All the code for this project, and most of the failed attempts, can be found on the `ai` branch of [Rustris](https://github.com/axle-h/rustris).
This has been a massive undertaking, so please if anyone has read this far and is interested then do check it out and give it a star ‚≠ê.
